{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are some commands to help you troubleshoot your setup\n",
    "# There is no action items that you need to bring up to the teaching staff\n",
    "# Remember, the job is for you to complete, we only provide you a potential\n",
    "# path, but you don't have to take it. We're also not going to help troubleshoot\n",
    "# installation issues. For that, feel free to use online resources depening\n",
    "# on the type of issue (or the application) that you are seeing.\n",
    "\n",
    "# Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "import torch\n",
    "import pybullet_envs\n",
    "import argparse\n",
    "import gfootball.env as football_env\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray import tune\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import tempfile\n",
    "import gym\n",
    "from gfootball import env as fe\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from IPython.display import HTML\n",
    "\n",
    "from rldm.utils import gif_tools as gt\n",
    "from rldm.utils import football_tools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We expect the following packages:\n",
    "\n",
    "# gfootball                     2.10.1\n",
    "# ray                           1.6.0\n",
    "# tensorflow                    2.6.0\n",
    "# tensorflow-estimator          2.6.0\n",
    "# tensorboard                   2.6.0\n",
    "# tensorboard-data-server       0.6.1\n",
    "# tensorboard-plugin-wit        1.8.0\n",
    "# tensorboardX                  2.4\n",
    "# torch                         1.9.0+cu111\n",
    "# torchaudio                    0.9.0\n",
    "# torchvision                   0.10.0+cu111\n",
    "# rldm                          1.0          /mnt\n",
    "\n",
    "# If you don't have the same packages, then maybe you don't have GPUs? Or you didn't use the Docker image provided?\n",
    "\n",
    "# No need to reach out if you don't have the same packages. This is only for your information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gfootball                     2.10.1\n",
      "ray                           1.6.0\n",
      "tensorflow                    2.6.0\n",
      "tensorflow-estimator          2.6.0\n",
      "tensorboard                   2.6.0\n",
      "tensorboard-data-server       0.6.1\n",
      "tensorboard-plugin-wit        1.8.0\n",
      "tensorboardX                  2.4\n",
      "torch                         1.9.0+cu111\n",
      "torchaudio                    0.9.0\n",
      "torchvision                   0.10.0+cu111\n",
      "rldm                          1.0          /mnt\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep -i gfootball\n",
    "!pip list | grep -i ray\n",
    "!pip list | grep -i tensorflow\n",
    "!pip list | grep -i tensorboard\n",
    "!pip list | grep -i torch\n",
    "!pip list | grep -i rldm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The command below will fail if you don't have a GPU.\n",
    "# There is no action item for you if you don't have a GPU and the command above fails.\n",
    "\n",
    "# If you do have a GPU and it is not showing on the output of the command above,\n",
    "# then something is wrong. Did you follow all the steps carefully? Are you passing the right\n",
    "# GPU command to docker when starting? These are some initial ideas for you to troubleshoot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 30 11:57:46 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA TITAN X ...  Off  | 00000000:08:00.0 Off |                  N/A |\r\n",
      "| 23%   29C    P8     9W / 250W |      2MiB / 12196MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA TITAN X ...  Off  | 00000000:41:00.0 Off |                  N/A |\r\n",
      "| 23%   35C    P8    17W / 250W |      2MiB / 12194MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of vCPUs on the system! Having lots of vCPUs can be\n",
    "# very helpful specially for this project. If you have multiple systems,\n",
    "# you may want to consider the system with the highest number of vCPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\r\n"
     ]
    }
   ],
   "source": [
    "!grep -c 'processor' /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For you to become aware of the memory available on your system.\n",
    "# If you are considering using complex models, such as models with\n",
    "# Recurrent cells, such as LSTMs, GRUs, or AttentionNets, then make\n",
    "# sure you have sufficient memory for your model. \"Sufficient\" here\n",
    "# depends on the actual model you end up using (number of params, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:          125Gi        55Gi       856Mi       2.0Mi        69Gi        68Gi\r\n",
      "Swap:          15Gi       0.0Ki        15Gi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The command below helps you become aware of the available\n",
    "# free space on your system. This can become important if you\n",
    "# are saving lots of checkpoints as you train. So make sure you\n",
    "# have enough for whatever you're planning to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "overlay         1.8T  775G  965G  45% /\r\n",
      "tmpfs            64M     0   64M   0% /dev\r\n",
      "tmpfs            63G     0   63G   0% /sys/fs/cgroup\r\n",
      "shm              64M     0   64M   0% /dev/shm\r\n",
      "/dev/md0        1.8T  775G  965G  45% /mnt\r\n",
      "tmpfs            63G   12K   63G   1% /proc/driver/nvidia\r\n",
      "/dev/nvme0n1p3  453G   24G  407G   6% /usr/bin/nvidia-smi\r\n",
      "dev              63G     0   63G   0% /dev/nvidia0\r\n",
      "tmpfs            63G     0   63G   0% /proc/asound\r\n",
      "tmpfs            63G     0   63G   0% /proc/acpi\r\n",
      "tmpfs            63G     0   63G   0% /proc/scsi\r\n",
      "tmpfs            63G     0   63G   0% /sys/firmware\r\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following command helps you verify\n",
    "# whether PyTorch is seeing your GPU or not.\n",
    "# It should say True if you do have a GPU and you\n",
    "# are using the provided Docker images, and if\n",
    "# you followed the instructions carefully\n",
    "\n",
    "# This is info for you only, to help you on your path\n",
    "# Feel free to try something else on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
