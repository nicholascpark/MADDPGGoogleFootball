episode_reward_max,episode_reward_min,episode_reward_mean,episode_len_mean,episodes_this_iter,num_healthy_workers,timesteps_total,agent_timesteps_total,done,episodes_total,training_iteration,experiment_id,date,timestamp,time_this_iter_s,time_total_s,pid,hostname,node_ip,time_since_restore,timesteps_since_restore,iterations_since_restore,trial_id,policy_reward_min/agent_0,policy_reward_min/agent_1,policy_reward_max/agent_0,policy_reward_max/agent_1,policy_reward_mean/agent_0,policy_reward_mean/agent_1,custom_metrics/game_result/num_pass_attempts_mean,custom_metrics/game_result/num_pass_attempts_min,custom_metrics/game_result/num_pass_attempts_max,custom_metrics/game_result/num_pass_success_mean,custom_metrics/game_result/num_pass_success_min,custom_metrics/game_result/num_pass_success_max,custom_metrics/game_result/score_reward_episode_mean,custom_metrics/game_result/score_reward_episode_min,custom_metrics/game_result/score_reward_episode_max,custom_metrics/game_result/win_percentage_episode_mean,custom_metrics/game_result/win_percentage_episode_min,custom_metrics/game_result/win_percentage_episode_max,hist_stats/episode_reward,hist_stats/episode_lengths,hist_stats/policy_agent_0_reward,hist_stats/policy_agent_1_reward,sampler_perf/mean_raw_obs_processing_ms,sampler_perf/mean_inference_ms,sampler_perf/mean_action_processing_ms,sampler_perf/mean_env_wait_ms,sampler_perf/mean_env_render_ms,timers/sample_time_ms,timers/sample_throughput,timers/load_time_ms,timers/load_throughput,timers/learn_time_ms,timers/learn_throughput,timers/update_time_ms,info/num_steps_sampled,info/num_agent_steps_sampled,info/num_steps_trained,info/num_agent_steps_trained,perf/cpu_util_percent,perf/ram_util_percent,perf/gpu_util_percent0,perf/vram_util_percent0,info/learner/agent_0/learner_stats/cur_kl_coeff,info/learner/agent_0/learner_stats/cur_lr,info/learner/agent_0/learner_stats/total_loss,info/learner/agent_0/learner_stats/policy_loss,info/learner/agent_0/learner_stats/vf_loss,info/learner/agent_0/learner_stats/vf_explained_var,info/learner/agent_0/learner_stats/kl,info/learner/agent_0/learner_stats/entropy,info/learner/agent_0/learner_stats/entropy_coeff,info/learner/agent_1/learner_stats/cur_kl_coeff,info/learner/agent_1/learner_stats/cur_lr,info/learner/agent_1/learner_stats/total_loss,info/learner/agent_1/learner_stats/policy_loss,info/learner/agent_1/learner_stats/vf_loss,info/learner/agent_1/learner_stats/vf_explained_var,info/learner/agent_1/learner_stats/kl,info/learner/agent_1/learner_stats/entropy,info/learner/agent_1/learner_stats/entropy_coeff
0.0,-2.0,-0.6,173.1,10,7,2800,5600,False,10,1,059898308a894a1eb71bc13221cf409e,2021-12-03_09-11-39,1638522699,71.56580924987793,71.56580924987793,15453,9679a517ecbf,172.17.0.2,71.56580924987793,0,1,48b69_00000,-1.0,-1.0,0.0,0.0,-0.3,-0.3,50.6,27,84,0.5,0,4,-0.3,-1,0,0.0,0,0,"[0.0, 0.0, 0.0, 0.0, -2.0, -2.0, 0.0, -2.0, 0.0, 0.0]","[256, 119, 83, 197, 92, 170, 218, 182, 102, 312]","[0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, -1.0, 0.0, 0.0]","[0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, -1.0, 0.0, 0.0]",1.9617942800545634,4.632051983973629,0.5350050486234061,24.17970536058383,0.0,38742.146,72.273,97.971,28579.897,31439.884,89.059,393.874,2800,5600,2800,5600,23.735714285714288,60.603571428571435,nan,0.3177722749255953,1.0,0.00022602718266055705,-0.016594612078430753,-0.023577334126457572,0.00939343148170987,-0.20664637,0.004912150873950933,2.9333736300468445,0.0004158966184268587,1.0,0.00022602718266055705,-0.01467442868029355,-0.02296951854285518,0.011619840724917941,-0.47086516,0.005445213052932647,2.9348562508821487,0.0004158966184268587
